{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14377147-bc91-4df7-9a98-2ddc2965460f",
   "metadata": {},
   "source": [
    "# Activation Functions \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc10411c-2b62-4705-8631-1a6b30722205",
   "metadata": {},
   "source": [
    "<br>\n",
    "An activation function is a function used in artificial neural networks which outputs a small value for small inputs, and a larger value if its inputs exceed a threshold. If the inputs are large enough, the activation function \"fires\", otherwise it does nothing. In other words, an activation function is like a gate that checks that an incoming value is greater than a critical number.\n",
    "\n",
    "Activation functions are useful because they add non-linearities into neural networks, allowing the neural networks to learn powerful operations. If the activation functions were to be removed from a feedforward neural network, the entire network could be re-factored to a simple linear operation or matrix transformation on its input, and it would no longer be capable of performing complex tasks such as image recognition.\n",
    "\n",
    "Well-known activation functions used in data science include the rectified linear unit (ReLU) function, and the family of sigmoid functions such as the logistic sigmoid function, the hyperbolic tangent, and the arctangent function\n",
    "\n",
    "    1. sigmoid function\n",
    "    2. step funtion\n",
    "    3. tanh function\n",
    "    \n",
    "    4. relu funtion\n",
    "    5. leaky relu function\n",
    "    \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align:center\"><img src='https://miro.medium.com/max/788/1*B3dckq_nbUlQruXA8qTSxg.png' style=\"background: #fff;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd6fde8-fcd7-4120-8b99-1d09e46b46f6",
   "metadata": {},
   "source": [
    "## 1. _step function_\n",
    "\n",
    "Taking the concept of the activation function to first principles, a single neuron in a neural network followed by an activation function can behave as a logic gate.\n",
    "\n",
    "Let us take the threshold step function as our activation function:\n",
    "\n",
    "<img src='https://images.deepai.org/user-content/0431006986-thumb-6084.svg' style=\"background: #fff;\"><br>\n",
    "\n",
    "Mathematical definition of the threshold step function, one of the simplest possible activation functions\n",
    "\n",
    "Binary step function is a threshold-based activation function which means after a certain threshold neuron is activated and below the said threshold neuron is deactivated. In the below graph, the threshold is zero. This activation function can be used in binary classifications as the name suggests, however it can not be used in a situation where you have multiple classes to deal with.\n",
    "\n",
    "\n",
    "<br><img src='https://images.deepai.org/user-content/5853361052-thumb-1150.svg' style=\"background: #fff;\">\n",
    "\n",
    "\n",
    "\n",
    "The threshold step function may have been the first activation function, introduced by Frank Rosenblatt while he was modeling biological neurons in 1962.\n",
    "\n",
    "And let us define a single layer neural network, also called a single layer perceptron, as:\n",
    "\n",
    "\n",
    "<br>\n",
    "<img src='https://images.deepai.org/user-content/6846233816-thumb-9076.svg' style=\"background: #fff;\">\n",
    "<br><img src='https://images.deepai.org/user-content/8031469793-thumb-4117.svg' style=\"background: #fff;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67a4dce-c090-4a57-8c23-b968e0c46e11",
   "metadata": {},
   "source": [
    "## 2. _sigmoid function_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6aa953-6087-4b4f-9f22-cbbedfa81125",
   "metadata": {},
   "source": [
    "A Sigmoid function is a mathematical function which has a characteristic S-shaped curve. There are a number of common sigmoid functions, such as the logistic function, the hyperbolic tangent, and the arctangent.<br>\n",
    "\n",
    "In machine learning, the term _sigmoid function_ is normally used to refer specifically to the logistic function, also called the logistic sigmoid function.\n",
    "All sigmoid functions have the property that they map the entire number line into a small range such as between 0 and 1, or -1 and 1, so one use of a sigmoid function is to convert a real value into one that can be interpreted as a probability.\n",
    "\n",
    "<br><img src='https://images.deepai.org/user-content/9279272907-thumb-1675.svg' style=\"background: #fff;\">\n",
    "\n",
    "Graph showing the characteristic S-shape of the logistic sigmoid function\n",
    "<br><br>\n",
    "\n",
    "<img src=\"https://images.deepai.org/user-content/1375463140-thumb-9221.svg\" style=\"background: #fff;\">\n",
    "\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a274799b-05f0-4ecd-a1aa-f853779b9d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97a1d21d-4d27-4f04-8c2a-ee8aa2a694ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005  :  0.5001249999973958\n",
      "0.1  :  0.52497918747894\n",
      "0.5  :  0.6224593312018546\n",
      "0.8  :  0.6899744811276125\n",
      "1  :  0.7310585786300049\n",
      "500  :  1.0\n",
      "1000  :  1.0\n",
      "10000  :  1.0\n"
     ]
    }
   ],
   "source": [
    "num = [0.0005, 0.1, 0.5, 0.8, 1, 500, 1000, 10000]\n",
    "\n",
    "for i in num:\n",
    "    print(i, \" : \", sigmoid(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e80c7d-b54d-4349-bc06-6255ec91ebd0",
   "metadata": {},
   "source": [
    "<br>\n",
    " However, Sigmoid function makes almost no change in the prediction for very high or very low inputs which ultimately results in neural network refusing to learn further, this problem is known as the vanishing gradient.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c3a882-eeec-4474-82f2-ffb4954d350e",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 3. _tanh function_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a26f668-c8a7-41b0-89cb-1efef1ec1d3c",
   "metadata": {},
   "source": [
    "Another common sigmoid function is the hyperbolic function. This maps any real-valued input to the range between -1 and 1.\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"https://images.deepai.org/user-content/1676244843-thumb-4292.svg\" style=\"background: #fff;\">\n",
    "<br>\n",
    "\n",
    "<img src=\"https://i0.wp.com/www.arshad-kazi.com/wp-content/uploads/2021/03/tanh.jpg?fit=512%2C284&ssl=1\" style=\"background: #fff;\">\n",
    "\n",
    "<br>\n",
    "\n",
    "Mathematical definition of the hyperbolic tangent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d30866e-105a-4b1b-925c-b2e6a0b2b651",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return ((math.exp(x)-math.exp(-x)) / (math.exp(x) + math.exp(-x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9179d7a7-ae4a-445d-bb85-0f0552138e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-500  :  -1.0\n",
      "-100  :  -1.0\n",
      "-50  :  -1.0\n",
      "-1  :  -0.7615941559557649\n",
      "0  :  0.0\n",
      "5  :  0.999909204262595\n",
      "100  :  1.0\n",
      "500  :  1.0\n"
     ]
    }
   ],
   "source": [
    "num = [-500, -100, -50, -1, 0, 5, 100, 500]\n",
    "\n",
    "for i in num:\n",
    "    print(i, \" : \", tanh(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f6e0e0-6e44-4f87-b943-0566a880752a",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "However, tanh also comes with the vanishing gradient problem just like sigmoid function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4491e954-919c-4d1d-a67a-55909b13a0ca",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 4. _relu(rectified linear activation function) function_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41ef63e-eac3-49a6-b33f-20099c0de9be",
   "metadata": {},
   "source": [
    "There are a number of widely used activation functions in deep learning today. One of the simplest is the rectified linear unit, or ReLU function, which is a piecewise linear function that outputs zero if its input is negative, and directly outputs the input otherwise:\n",
    "\n",
    "<img src=\"https://images.deepai.org/user-content/1128419011-thumb-5497.svg\" style=\"background: #fff;\">\n",
    "\n",
    "Mathematical definition of the ReLU Function\n",
    "\n",
    "<img src=\"https://images.deepai.org/user-content/4015736703-thumb-4932.svg\" style=\"background: #fff;\">\n",
    "\n",
    "<br>\n",
    "Graph of the ReLU function, showing its flat gradient for negative x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8df94d26-0920-4285-8b86-add5673da8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return max(0,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c49ec1c9-1703-431e-aceb-52bb26bddd8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-500  :  0\n",
      "-100  :  0\n",
      "-50  :  0\n",
      "-1  :  0\n",
      "0  :  0\n",
      "5  :  5\n",
      "100  :  100\n",
      "500  :  500\n"
     ]
    }
   ],
   "source": [
    "for i in num:\n",
    "    print(i, \" : \", relu(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c24382-150e-4917-9850-cd29d065b0b0",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "when the input is zero or a negative value, the function outputs zero and it hinders with the back-propagation. This problem is known as the dying ReLU problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93918a6-d61e-4c6f-a33a-3010cf677c8e",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 5. _leaky relu function_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62033d65-8fac-44d4-ab2f-ea8a74dd32da",
   "metadata": {},
   "source": [
    "Leaky ReLU prevents the dying ReLU problem and enable back-propagation. One flaw of Leaky ReLU is the slope being predetermined rather than letting the neural network figure it out.\n",
    "\n",
    "\n",
    "<img src=\"https://www.i2tutorials.com/wp-content/media/2019/09/Deep-learning-25-i2tutorials.png\" style=\"background: #fff;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a0b2bd4-6542-42f3-a526-f7a0a9ac5be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(x):\n",
    "    return max(0.1*x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6bc1ec5-e9d4-400c-b066-39b3ba2bd259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-500  :  -50.0\n",
      "-100  :  -10.0\n",
      "-50  :  -5.0\n",
      "-1  :  -0.1\n",
      "0  :  0.0\n",
      "5  :  5\n",
      "100  :  100\n",
      "500  :  500\n"
     ]
    }
   ],
   "source": [
    "for i in num:\n",
    "    print(i, \" : \", leaky_relu(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43ba9ee-9a1f-4c42-8c64-abb412960f03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
